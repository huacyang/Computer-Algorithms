## Approximation Algorithm

approximation ratio (minimization):  
&sigma;<sub>a</sub> = max<sub>i</sub> A(I) / OPT(I)

__Example.__ Vertex Cover  
* __Set Cover.__ You have _n_ elements and _k_ subsets. You want to select the minimum number of subsets so that all elements are covered by the selection
* __Vertex Cover.__ Instance of set cover
  - Elements: edges of a graph
  - Subsets: are defined by the edges incident to each vertex (one subset per vertex)
  - Uses the Greedy Approach

Let's see a better approx. algorithm than the greedy approach.  
* __Matching notion.__ Subset of edges that have no vertices in common.
* __Maximal matching.__ No new edges can be added.

Relationship with Vertex Cover.  
* Any vertex cover has at least as many vertices as the number of edges in a maximal matching.
* Why? Each edge of the maximal matching must be covered by an individual vertex in the vertex cover.
* If I select all the vertices incident to edges in a maximal matching, then I have a vertex cover.
* Why? If there is an edge that is not covered, then this edge would be incident to the maximal matching as it doesn't share a vertex with any edge in the M.M. Cnotradication!

Polynomial algorithm for returning a vertex cover of size (2*number of edges in MM).  
Lower bound for optimal vertex cover: # of edges in maximal matching.

## Clustering  
* Data set x = {x<sub>1</sub>,...,x<sub>n</sub>}
* Divide the elemetns of _x_ into _k_ clusters in terms of similarity
* For each pair of elements you hav a distance d(x,y)
  - d(x,y) &ge; 0
  - d(x,y) &ge; d(y,x)
  - d(x,x) = 0
  - d(x,z) &le; d(&lamda;,y) + d(y,z)
* __Upper bound__ for diameter according to k-means solution is _2r_ and __lower bound__ for optimum diameter is _r_
    - x = 2

## Notion of Similarity
Diameter of a cluster G<sub>j</sub>  
* D<sub>j</sub> = max<sub>x,y &isin; j</sub> d(x,y)
* Objective is to minimize the maximum cluster diameter: min max<sub>j</sub> D<sub>j</sub>

Example: Planar points  
* Think clusters as spheres for which you want to minimize their radius but which cover all elements.

NP-hard problem but an approx. algorithm  
* __Idea.__ If you have _k_ cluster centers, then each element should be assigned to the closest cluster center according to the distance.
* __2<sup>nd</sup> Idea.__ After selecting a set of cluster centers, the next one should be as far as possible from its closest center.
* It turns out that for this algorithm, the diameter is at most twice the optimum.

__Example.__ Consider the point x &isin; X furthest away from the _k_ cluster centers: u<sub>1</sub>,...,u<sub>k</sub>  
* _r_. distance between _x_ and the closest cluster center
* Every point has to be within distance _r_ from its closest cluster center.
* Consider only the elements u<sub>1</sub>,...,u<sub>k</sub>, the distance between all the elements in this set is at least _r_ .
* If we clustered these _k+1_ elements into _k_ clusters, we would get _k-1_ clusters with one element, and 1 cluster with 2 elements.
* The diameter of this cluster (i.e. the diameter of the clustering is at least r)

## Traveling Salesman Problem
* Assume that you can transition between any pair of vertices (complete graph) and that the distances satisfy satisfy the metric properties.
* __Observation.__ Removing one edge from a TSP route gives a spanning tree
  - TSP<sub>cost</sub> &ge; cost of corresponding spanning tree &ge; MST<sub>cost</sub>
* __Approach.__ Traverse all the edges of the MST twice so as to visit every vertex and return to the initial one
* __Cost.__
  - MST<sub>tour</sub> = 2MST &le; 2TSP
* __Idea.__ Every time you would revisit a vertex, skip directly to the next one accordingly to the MST tour.
  - Because of the triangular inequality the edge youare adding to the tour has lower cost than the edges you are replacing (A &rarr; C is shorter than A &rarr; B &rarr; C).
